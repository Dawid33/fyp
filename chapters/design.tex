\chapter{Design} \label{design}

\section{Lexer}
Lexers are defined using \glspl{reg_exp} that describe the character sequences
which need to be found and emitted from the lexer as lexemes. These regular
expressions can be used to construct a \gls{dfa} that accepts the source
code as input and is capable of recognising whether the input matches any of
the original regular expressions. In order to construct the \gls{dfa}, it is
necessary to convert the regular expression into a \gls{nfa} using Thompson's
construction algorithm, which results in a graph where nodes represent the
states within the \gls{nfa} and the edges represent possible transitions that
can be caused by input symbols. This \gls{nfa} can then be converted into a
\gls{dfa} through the powerset construction algorithm. 

% Insert a figure here like the one in data-parallel finite state machines.

Instead of using the \gls{dfa} directly when lexing, it is more efficient
to instead keep track of the current state in a variable and compute the
state transition by using a lookup table. This state transition table has the
potential input symbols on one axis and potential states the lexer can be in on
the other axis. The table can be constructed by travering the \gls{dfa} from its
initial state and populating it the states to which the lexer can transition to.

% During generation, look for duplicate state transitions where the characters
% are the same, but states differ. This indicates that those states are potential
% start states of the lexer.

In order to lex the source code in parallel, the source code is split into
roughly equal sized parts. These parts are put onto a queue that can be
concurrently accessed by a thread pool. Each thread then takes a part of the
source code and tokenizes from every possible initial state, resultling in as
many outputs as there are possible initial states. These outputs are collected
in a concurrently accessible array until all parts of the source code are
tokenized. Once the work queue is emtpy, the correct output from each part is
chosen such that its start state is the same as the previous chunk's finish
state. Remaining outputs are discarded. This list of token lists, reprsenting
the tokenized version of the input source code, is then passed on to the parser.

\section{Parser}
\begin{itemize}
	\item Read grammar from file that defines terminals, non-terminals nad production rules.
	\item Transform grammar into floyd normal form as defined in \cite{barenghi_parallel_2015}
	\item Build operator precedence table according to \cite{grune_parsing_2008}
	\item Take the lexer output from the previous step and parse each chunk using the same
  		  thread pool approach. The parsing algorithm returns a partial parse tree and a stack of lexemes.
	\item Take the outputs from each chunk and join the partial parse trees using
  		  the same parsing algorithm.
\end{itemize}

\cite{barenghi_parallel_2015} explains an algorithm for parsing operator
precedence grammars. An operator precedence grammar is a kind of context-free
grammar  where the right hand-side of each production rule contains at least
one terminal or non-terminal and no right-hand side contains two consecutive
non-terminals. In order to parse such a grammar using the algorithm described by
\cite{barenghi_parallel_2015}, it is necessary to transform the grammar into a
floyd normal form.

Once the initial grammar has been transformed into a bounded context grammar, it
can then be parsed by the algorithm described by \cite{barenghi_parallel_2015}.
This algorithm requires the construction of a standard operator precedence table
that contains the precedence relationships between terminals in the grammar.

\subsection{Generating the Operator Precedence Table}

Setting up the precedence table for such grammars is a relatively
straightforward process. Initially, we determine the set FIRSTOP(A) for each
non-terminal A, representing the operators that can appear as the first operator
in sentential forms derived from A. It's important to note that this initial
operator can be preceded by at most one non-terminal in an operator grammar. The
construction of FIRSTOP sets for all non-terminals occurs simultaneously through
the following steps:

\begin{enumerate}

	\item For each non-terminal A, identify all right-hand sides of its rules.
	For each right-hand side R, include the first operator in R (if present) in
	FIRSTOP(A). This step establishes the initial values for all FIRSTOP sets.

    \item For each non-terminal A, examine all right-hand sides of its rules.
For each right-hand side R that commences with a non-terminal, denoted as
B, incorporate the elements of FIRSTOP(B) into FIRSTOP(A). This step is
justified by the possibility that a sentential form of A may initiate with B,
necessitating the inclusion of all operators in FIRSTOP(B) into FIRSTOP(A).

    \item Iteratively repeat step 2 until no further changes occur in any
FIRSTOP set. At this point, we have determined the FIRSTOP sets for all
non-terminals.
\end{enumerate}

Additionally, we require the set LASTOP(A), defined in a similar manner. An analogous algorithm involves utilizing the last operator in R during step 1 and incorporating the elements of FIRSTOP(B) where B terminates A in step 2. These steps facilitate the determination of LASTOP sets.


\section{Semantic Analysis}
\begin{itemize}
	\item Iterate over the parse tree in post-order DFS in order to generate the Abstract syntax tree.
	\item Iterate over AST using pre-order DFS, creating a new thread and symbol table for certain nodes.
	\item Check if variables are defined before they are used by checking if they exist in ancestor symbol tables.
\end{itemize}



